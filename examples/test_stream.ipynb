{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b973e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import StructuredMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03240dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GLM_API_KEY\", \"your_api_key_here\")\n",
    "base_url = os.getenv(\"GLM_BASE_URL\", \"your_api_base_url_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e70175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c2dbfa218c1f3a8c11432259644c5913.QCQk336aSIsrJ94L',\n",
       " 'https://open.bigmodel.cn/api/paas/v4/')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(api_key, base_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d716f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent that uses the OpenAI GPT-4o model.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "\n",
    "    model=\"glm-4.5\",\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    model_info={\n",
    "        'vision': False,\n",
    "        'function_calling': True,\n",
    "        'json_output': True,\n",
    "        'structured_output': False,\n",
    "        'family': \"glm\",\n",
    "    },\n",
    ")\n",
    "\n",
    "model_client = OllamaChatCompletionClient(\n",
    "            model=\"qwen3:30b\",\n",
    "            model_info={\n",
    "                'vision': False,\n",
    "                'function_calling': True,\n",
    "                'json_output': True,\n",
    "                'structured_output': False,\n",
    "                'family': \"qwen\",\n",
    "            },\n",
    "            options={\n",
    "                'num_ctx': int(os.getenv(\"NUM_CTX\", \"6000\")),\n",
    "                'stream': True,  # 开启流式输出\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0ab750",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    model_client_stream=True,  # Enable streaming tokens.\n",
    " )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82300d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#流式输出streaming_assistant的内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "#model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client, model_client_stream=True)  # Enable streaming tokens.\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
    "termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# Create the team.\n",
    "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
    "\n",
    "# Run the conversation and stream to the console.\n",
    "stream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n",
    "# Use asyncio.run(...) when running in a script.\n",
    "async for message in stream: #streaming_assistant.run_stream(task=\"Name two cities in South America\"):  # type: ignore\n",
    "    print(message.content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def assistant_run_stream() -> None:\n",
    "    # Option 1: read each message from the stream (as shown in the previous example).\n",
    "    # async for message in agent.run_stream(task=\"Find information on AutoGen\"):\n",
    "    #     print(message)\n",
    "\n",
    "    # Option 2: use Console to print all messages as they appear.\n",
    "    await Console(\n",
    "        agent.run_stream(task=\"Find information on AutoGen\"),\n",
    "        output_stats=False,  # Enable stats printing.\n",
    "    )\n",
    "\n",
    "\n",
    "# Use asyncio.run(assistant_run_stream()) when running in a script.\n",
    "await assistant_run_stream()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
