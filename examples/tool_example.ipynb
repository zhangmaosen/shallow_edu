{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae428abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import StructuredMessage\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5b4a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"GLM_API_KEY\", \"your_api_key_here\")\n",
    "base_url = os.getenv(\"GLM_BASE_URL\", \"your_api_base_url_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2d02e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(\n",
    "\n",
    "    model=\"glm-4.5\",\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    model_info={\n",
    "        'vision': False,\n",
    "        'function_calling': True,\n",
    "        'json_output': True,\n",
    "        'structured_output': False,\n",
    "        'family': \"glm\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b1779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool that searches the web for information.\n",
    "# For simplicity, we will use a mock function here that returns a static string.\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Find information on the web\"\"\"\n",
    "    return \"AutoGen is a programming framework for building multi-agent applications.\"\n",
    "\n",
    "async def save_to_file(content: str, filename: str) -> str:\n",
    "    \"\"\"Save content to a file\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    tools=[web_search, save_to_file],\n",
    "    system_message=\"Use tools to solve tasks.\",\n",
    "    model_client_stream=True,  # Enable streaming tokens.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa9e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "save the information about autogen to file named 'info.txt'\n",
      "---------- ModelClientStreamingChunkEvent (assistant) ----------\n",
      "<think>\n",
      "The user wants me to save information about autogen to a file named 'info.txt'. I just searched for information about autogen and found that \"AutoGen is a programming framework for building multi-agent applications.\" I should now save this information to the file as requested.</think>\n",
      "\n",
      "---------- ToolCallRequestEvent (assistant) ----------\n",
      "[FunctionCall(id='call_CxuerVhDSSO8gAgruCQYgA', arguments='{\"content\": \"AutoGen is a programming framework for building multi-agent applications.\", \"filename\": \"info.txt\"}', name='save_to_file')]\n",
      "---------- ToolCallExecutionEvent (assistant) ----------\n",
      "[FunctionExecutionResult(content='None', name='save_to_file', call_id='call_CxuerVhDSSO8gAgruCQYgA', is_error=False)]\n",
      "---------- ToolCallSummaryMessage (assistant) ----------\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content=\"save the information about autogen to file named 'info.txt'\", type='TextMessage'), ThoughtEvent(source='assistant', models_usage=None, metadata={}, content='\\n', type='ThoughtEvent'), ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=359, completion_tokens=93), metadata={}, content=[FunctionCall(id='call_CxuerVhDSSO8gAgruCQYgA', arguments='{\"content\": \"AutoGen is a programming framework for building multi-agent applications.\", \"filename\": \"info.txt\"}', name='save_to_file')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='None', name='save_to_file', call_id='call_CxuerVhDSSO8gAgruCQYgA', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant', models_usage=None, metadata={}, content='None', type='ToolCallSummaryMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Console(\n",
    "        agent.run_stream(task=\"save the information about autogen to file named 'info.txt'\"),\n",
    "        output_stats=False,  # Enable stats printing.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d681f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    model_client_stream=True,  # Enable streaming tokens.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddf9a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始流式输出 streaming_assistant 的结果...\n",
      "==================================================\n",
      "方法1: 使用 Console 显示流式输出\n",
      "---------- TextMessage (user) ----------\n",
      "简要介绍人工智能的发展历史\n",
      "---------- ModelClientStreamingChunkEvent (assistant) ----------\n",
      "<think>嗯，用户让我简要介绍人工智能的发展历史。这个问题看起来挺基础的，但“简要”二字意味着需要提炼关键节点，不能太冗长。用户可能是学生做作业，或者对AI感兴趣的普通读者，想快速了解脉络。\n",
      "\n",
      "从专业角度，AI发展史通常分几个阶段：早期萌芽（1950s）、第一次黄金期（1956-1974）、第一次AI寒冬（1974-1980）、专家系统兴起（1980-1987）、第二次寒冬（1987-1993）、统计学习崛起（1990s-2010s）、深度学习革命（2012至今）。不过用户要“简要”，得压缩成3-4个核心阶段。\n",
      "\n",
      "用户没提具体用途，但“简要”暗示需要时间线清晰。我决定按“诞生-黄金期-寒冬-复兴”四段式，重点突出图灵测试、达特茅斯会议、专家系统、深度学习这几个里程碑。最后加一句现状总结，让用户知道AI现在处于什么位置。\n",
      "\n",
      "要注意避免术语轰炸。比如提到“符号主义”时得简单解释，但用户要“简要”，所以用“基于逻辑推理”这种更直白的说法。深度学习部分必须提AlphaGo，这是大众最熟悉的案例。\n",
      "\n",
      "对了，用户可能对“寒冬”概念陌生，需要说明是资金和兴趣低谷期。最后用“从实验室走向日常生活”收尾，让历史脉络有现实意义——毕竟用户可能更关心AI为什么现在这么火。</think>人工智能（AI）的发展历史可概括为以下几个关键阶段，经历了多次起伏与突破：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 萌芽期（1940s–1950s）：理论奠基**\n",
      "- **1943年**：神经科学家麦卡洛克（McCulloch）和数学家皮茨（Pitts）提出首个神经元数学模型，模拟人脑工作原理。\n",
      "- **1950年**：图灵（Alan Turing）提出“图灵测试”，定义机器智能的判断标准，奠定AI哲学基础。\n",
      "- **1956年**：**达特茅斯会议**（Dartmouth Workshop）正式确立“人工智能”（Artificial Intelligence）这一学科名称，标志着AI诞生。约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Minsky）等先驱参会。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 第一次黄金期（1956–1974）：乐观探索**\n",
      "- **早期成果**：\n",
      "  - **逻辑理论家**（1956）：纽厄尔（Newell）和司马贺（Simon）开发首个AI程序，证明数学定理。\n",
      "  - **ELIZA**（1966）：约瑟夫·维森鲍姆（Joseph Weizenbaum）创建聊天机器人，模拟心理治疗师对话。\n",
      "- **研究方向**：符号主义（Symbolic AI）主导，通过逻辑规则和知识库模拟人类推理。\n",
      "- **局限**：计算能力不足，数据匮乏，对复杂问题（如自然语言理解）处理能力有限。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 第一次AI寒冬（1974–1980）：现实打击**\n",
      "- **原因**：\n",
      "  - **技术瓶颈**：机器无法处理不确定性问题（如图像识别、语音理解）。\n",
      "  - **资金削减**：英国、美国等政府停止对大型AI项目的资助（如英国莱特希尔报告批评AI进展缓慢）。\n",
      "- **反思**：研究者意识到需更关注实际应用，而非纯理论。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 专家系统时代（1980–1987）：商业应用兴起**\n",
      "- **突破**：**专家系统**（Expert Systems）成为主流，通过预设规则库解决特定领域问题（如医疗诊断MYCIN、化学分析DENDRAL）。\n",
      "- **产业繁荣**：日本启动“第五代计算机”计划，美国企业（如Symbolics）投入AI研发。\n",
      "- **局限**：规则库维护成本高，缺乏学习能力，难以适应动态环境。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 第二次AI寒冬（1987–1993）：泡沫破裂**\n",
      "- **原因**：\n",
      "  - 专家系统维护复杂，通用性差，商业价值未达预期。\n",
      "  - 计算硬件成本高，个人计算机普及冲击专用AI设备市场。\n",
      "- **影响**：AI研究转向低调，学术资金锐减。\n",
      "\n",
      "---\n",
      "\n",
      "### **6. 统计学习与复兴（1990s–2010s）：数据驱动革命**\n",
      "- **关键转折**：\n",
      "  - **机器学习崛起**：从规则转向数据驱动算法（如决策树、支持向量机SVM）。\n",
      "  - **互联网爆发**：海量数据提供训练基础，算力提升（GPU加速）。\n",
      "  - **里程碑事件**：\n",
      "    - **1997年**：IBM深蓝（Deep Blue）击败国际象棋世界冠军卡斯帕罗夫。\n",
      "    - **2011年**：IBM Watson在智力问答节目《危险边缘》中战胜人类冠军。\n",
      "- **理论突破**：深度学习基础奠定（如反向传播算法、卷积神经网络CNN）。\n",
      "\n",
      "---\n",
      "\n",
      "### **7. 深度学习时代（2012至今）：爆发式增长**\n",
      "- **2012年**：AlexNet（基于CNN）在ImageNet图像识别大赛中准确率大幅领先，标志**深度学习革命**开始。\n",
      "- **技术突破**：\n",
      "  - **自然语言处理（NLP）**：Transformer架构（2017）推动BERT、GPT等大语言模型（LLM）发展。\n",
      "  - **强化学习**：DeepMind的AlphaGo（2016）击败围棋世界冠军李世石。\n",
      "  - **生成式AI**：DALL·E、MidJourney（图像生成），ChatGPT（对话AI）展现强大创造力。\n",
      "- **现状**：\n",
      "  - **应用普及**：AI融入医疗、金融、交通、教育等领域（如自动驾驶、药物研发）。\n",
      "  - **挑战与争议**：伦理问题（偏见、隐私）、就业影响、技术安全（如AI对齐）引发全球讨论。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "AI的发展是从**逻辑推理**到**数据驱动**，从**规则编程**到**自主学习**的演进过程。历经两次寒冬的沉淀，如今在算力、算法和数据的共同推动下，AI正从实验室走向日常生活，成为新一轮科技革命的核心引擎。未来，通用人工智能（AGI）的实现仍是终极目标，但需平衡技术进步与伦理治理。\n",
      "---------- TextMessage (assistant) ----------\n",
      "人工智能（AI）的发展历史可概括为以下几个关键阶段，经历了多次起伏与突破：\n",
      "\n",
      "---\n",
      "\n",
      "### **1. 萌芽期（1940s–1950s）：理论奠基**\n",
      "- **1943年**：神经科学家麦卡洛克（McCulloch）和数学家皮茨（Pitts）提出首个神经元数学模型，模拟人脑工作原理。\n",
      "- **1950年**：图灵（Alan Turing）提出“图灵测试”，定义机器智能的判断标准，奠定AI哲学基础。\n",
      "- **1956年**：**达特茅斯会议**（Dartmouth Workshop）正式确立“人工智能”（Artificial Intelligence）这一学科名称，标志着AI诞生。约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Minsky）等先驱参会。\n",
      "\n",
      "---\n",
      "\n",
      "### **2. 第一次黄金期（1956–1974）：乐观探索**\n",
      "- **早期成果**：\n",
      "  - **逻辑理论家**（1956）：纽厄尔（Newell）和司马贺（Simon）开发首个AI程序，证明数学定理。\n",
      "  - **ELIZA**（1966）：约瑟夫·维森鲍姆（Joseph Weizenbaum）创建聊天机器人，模拟心理治疗师对话。\n",
      "- **研究方向**：符号主义（Symbolic AI）主导，通过逻辑规则和知识库模拟人类推理。\n",
      "- **局限**：计算能力不足，数据匮乏，对复杂问题（如自然语言理解）处理能力有限。\n",
      "\n",
      "---\n",
      "\n",
      "### **3. 第一次AI寒冬（1974–1980）：现实打击**\n",
      "- **原因**：\n",
      "  - **技术瓶颈**：机器无法处理不确定性问题（如图像识别、语音理解）。\n",
      "  - **资金削减**：英国、美国等政府停止对大型AI项目的资助（如英国莱特希尔报告批评AI进展缓慢）。\n",
      "- **反思**：研究者意识到需更关注实际应用，而非纯理论。\n",
      "\n",
      "---\n",
      "\n",
      "### **4. 专家系统时代（1980–1987）：商业应用兴起**\n",
      "- **突破**：**专家系统**（Expert Systems）成为主流，通过预设规则库解决特定领域问题（如医疗诊断MYCIN、化学分析DENDRAL）。\n",
      "- **产业繁荣**：日本启动“第五代计算机”计划，美国企业（如Symbolics）投入AI研发。\n",
      "- **局限**：规则库维护成本高，缺乏学习能力，难以适应动态环境。\n",
      "\n",
      "---\n",
      "\n",
      "### **5. 第二次AI寒冬（1987–1993）：泡沫破裂**\n",
      "- **原因**：\n",
      "  - 专家系统维护复杂，通用性差，商业价值未达预期。\n",
      "  - 计算硬件成本高，个人计算机普及冲击专用AI设备市场。\n",
      "- **影响**：AI研究转向低调，学术资金锐减。\n",
      "\n",
      "---\n",
      "\n",
      "### **6. 统计学习与复兴（1990s–2010s）：数据驱动革命**\n",
      "- **关键转折**：\n",
      "  - **机器学习崛起**：从规则转向数据驱动算法（如决策树、支持向量机SVM）。\n",
      "  - **互联网爆发**：海量数据提供训练基础，算力提升（GPU加速）。\n",
      "  - **里程碑事件**：\n",
      "    - **1997年**：IBM深蓝（Deep Blue）击败国际象棋世界冠军卡斯帕罗夫。\n",
      "    - **2011年**：IBM Watson在智力问答节目《危险边缘》中战胜人类冠军。\n",
      "- **理论突破**：深度学习基础奠定（如反向传播算法、卷积神经网络CNN）。\n",
      "\n",
      "---\n",
      "\n",
      "### **7. 深度学习时代（2012至今）：爆发式增长**\n",
      "- **2012年**：AlexNet（基于CNN）在ImageNet图像识别大赛中准确率大幅领先，标志**深度学习革命**开始。\n",
      "- **技术突破**：\n",
      "  - **自然语言处理（NLP）**：Transformer架构（2017）推动BERT、GPT等大语言模型（LLM）发展。\n",
      "  - **强化学习**：DeepMind的AlphaGo（2016）击败围棋世界冠军李世石。\n",
      "  - **生成式AI**：DALL·E、MidJourney（图像生成），ChatGPT（对话AI）展现强大创造力。\n",
      "- **现状**：\n",
      "  - **应用普及**：AI融入医疗、金融、交通、教育等领域（如自动驾驶、药物研发）。\n",
      "  - **挑战与争议**：伦理问题（偏见、隐私）、就业影响、技术安全（如AI对齐）引发全球讨论。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "AI的发展是从**逻辑推理**到**数据驱动**，从**规则编程**到**自主学习**的演进过程。历经两次寒冬的沉淀，如今在算力、算法和数据的共同推动下，AI正从实验室走向日常生活，成为新一轮科技革命的核心引擎。未来，通用人工智能（AGI）的实现仍是终极目标，但需平衡技术进步与伦理治理。\n",
      "[Prompt tokens: 18, Completion tokens: 1346]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: None\n",
      "Total prompt tokens: 18\n",
      "Total completion tokens: 1346\n",
      "Duration: 33.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='简要介绍人工智能的发展历史', type='TextMessage'), ThoughtEvent(source='assistant', models_usage=None, metadata={}, content='嗯，用户让我简要介绍人工智能的发展历史。这个问题看起来挺基础的，但“简要”二字意味着需要提炼关键节点，不能太冗长。用户可能是学生做作业，或者对AI感兴趣的普通读者，想快速了解脉络。\\n\\n从专业角度，AI发展史通常分几个阶段：早期萌芽（1950s）、第一次黄金期（1956-1974）、第一次AI寒冬（1974-1980）、专家系统兴起（1980-1987）、第二次寒冬（1987-1993）、统计学习崛起（1990s-2010s）、深度学习革命（2012至今）。不过用户要“简要”，得压缩成3-4个核心阶段。\\n\\n用户没提具体用途，但“简要”暗示需要时间线清晰。我决定按“诞生-黄金期-寒冬-复兴”四段式，重点突出图灵测试、达特茅斯会议、专家系统、深度学习这几个里程碑。最后加一句现状总结，让用户知道AI现在处于什么位置。\\n\\n要注意避免术语轰炸。比如提到“符号主义”时得简单解释，但用户要“简要”，所以用“基于逻辑推理”这种更直白的说法。深度学习部分必须提AlphaGo，这是大众最熟悉的案例。\\n\\n对了，用户可能对“寒冬”概念陌生，需要说明是资金和兴趣低谷期。最后用“从实验室走向日常生活”收尾，让历史脉络有现实意义——毕竟用户可能更关心AI为什么现在这么火。', type='ThoughtEvent'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=18, completion_tokens=1346), metadata={}, content='人工智能（AI）的发展历史可概括为以下几个关键阶段，经历了多次起伏与突破：\\n\\n---\\n\\n### **1. 萌芽期（1940s–1950s）：理论奠基**\\n- **1943年**：神经科学家麦卡洛克（McCulloch）和数学家皮茨（Pitts）提出首个神经元数学模型，模拟人脑工作原理。\\n- **1950年**：图灵（Alan Turing）提出“图灵测试”，定义机器智能的判断标准，奠定AI哲学基础。\\n- **1956年**：**达特茅斯会议**（Dartmouth Workshop）正式确立“人工智能”（Artificial Intelligence）这一学科名称，标志着AI诞生。约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Minsky）等先驱参会。\\n\\n---\\n\\n### **2. 第一次黄金期（1956–1974）：乐观探索**\\n- **早期成果**：\\n  - **逻辑理论家**（1956）：纽厄尔（Newell）和司马贺（Simon）开发首个AI程序，证明数学定理。\\n  - **ELIZA**（1966）：约瑟夫·维森鲍姆（Joseph Weizenbaum）创建聊天机器人，模拟心理治疗师对话。\\n- **研究方向**：符号主义（Symbolic AI）主导，通过逻辑规则和知识库模拟人类推理。\\n- **局限**：计算能力不足，数据匮乏，对复杂问题（如自然语言理解）处理能力有限。\\n\\n---\\n\\n### **3. 第一次AI寒冬（1974–1980）：现实打击**\\n- **原因**：\\n  - **技术瓶颈**：机器无法处理不确定性问题（如图像识别、语音理解）。\\n  - **资金削减**：英国、美国等政府停止对大型AI项目的资助（如英国莱特希尔报告批评AI进展缓慢）。\\n- **反思**：研究者意识到需更关注实际应用，而非纯理论。\\n\\n---\\n\\n### **4. 专家系统时代（1980–1987）：商业应用兴起**\\n- **突破**：**专家系统**（Expert Systems）成为主流，通过预设规则库解决特定领域问题（如医疗诊断MYCIN、化学分析DENDRAL）。\\n- **产业繁荣**：日本启动“第五代计算机”计划，美国企业（如Symbolics）投入AI研发。\\n- **局限**：规则库维护成本高，缺乏学习能力，难以适应动态环境。\\n\\n---\\n\\n### **5. 第二次AI寒冬（1987–1993）：泡沫破裂**\\n- **原因**：\\n  - 专家系统维护复杂，通用性差，商业价值未达预期。\\n  - 计算硬件成本高，个人计算机普及冲击专用AI设备市场。\\n- **影响**：AI研究转向低调，学术资金锐减。\\n\\n---\\n\\n### **6. 统计学习与复兴（1990s–2010s）：数据驱动革命**\\n- **关键转折**：\\n  - **机器学习崛起**：从规则转向数据驱动算法（如决策树、支持向量机SVM）。\\n  - **互联网爆发**：海量数据提供训练基础，算力提升（GPU加速）。\\n  - **里程碑事件**：\\n    - **1997年**：IBM深蓝（Deep Blue）击败国际象棋世界冠军卡斯帕罗夫。\\n    - **2011年**：IBM Watson在智力问答节目《危险边缘》中战胜人类冠军。\\n- **理论突破**：深度学习基础奠定（如反向传播算法、卷积神经网络CNN）。\\n\\n---\\n\\n### **7. 深度学习时代（2012至今）：爆发式增长**\\n- **2012年**：AlexNet（基于CNN）在ImageNet图像识别大赛中准确率大幅领先，标志**深度学习革命**开始。\\n- **技术突破**：\\n  - **自然语言处理（NLP）**：Transformer架构（2017）推动BERT、GPT等大语言模型（LLM）发展。\\n  - **强化学习**：DeepMind的AlphaGo（2016）击败围棋世界冠军李世石。\\n  - **生成式AI**：DALL·E、MidJourney（图像生成），ChatGPT（对话AI）展现强大创造力。\\n- **现状**：\\n  - **应用普及**：AI融入医疗、金融、交通、教育等领域（如自动驾驶、药物研发）。\\n  - **挑战与争议**：伦理问题（偏见、隐私）、就业影响、技术安全（如AI对齐）引发全球讨论。\\n\\n---\\n\\n### **总结**\\nAI的发展是从**逻辑推理**到**数据驱动**，从**规则编程**到**自主学习**的演进过程。历经两次寒冬的沉淀，如今在算力、算法和数据的共同推动下，AI正从实验室走向日常生活，成为新一轮科技革命的核心引擎。未来，通用人工智能（AGI）的实现仍是终极目标，但需平衡技术进步与伦理治理。', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"开始流式输出 streaming_assistant 的结果...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 方法1: 使用 Console 显示流式输出（推荐）\n",
    "print(\"方法1: 使用 Console 显示流式输出\")\n",
    "await Console(\n",
    "    streaming_assistant.run_stream(task=\"简要介绍人工智能的发展历史\"),\n",
    "    output_stats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e210c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Write a 4-line poem about the ocean.\n",
      "---------- ThoughtEvent (assistant) ----------\n",
      "I need to write a 4-line poem about the ocean. This is a creative writing task that doesn't require any specific tools. I can compose this poem directly.\n",
      "\n",
      "Let me think about what aspects of the ocean I could include in a short 4-line poem:\n",
      "- The vastness and depth\n",
      "- The waves and tides\n",
      "- The marine life\n",
      "- The colors (blue, green, etc.)\n",
      "- The sounds of the ocean\n",
      "- The feeling of being near the ocean\n",
      "\n",
      "I'll try to create a concise but evocative 4-line poem that captures some essence of the ocean:\n",
      "\n",
      "```\n",
      "Vast blue waters stretch beyond sight,\n",
      "Waves crash with power, day and night.\n",
      "Life teems beneath the surface deep,\n",
      "An ancient secret that the oceans keep.\n",
      "```\n",
      "\n",
      "This poem has 4 lines, each with a different aspect of the ocean - its vastness, its movement, its life, and its mystery. I think this works well for the task.\n",
      "---------- TextMessage (assistant) ----------\n",
      "The ocean's waves in blue-green splendor,\n",
      "Crashing on the shore without surrender.\n",
      "Holding secrets in its depths so deep,\n",
      "Where ancient mysteries forever sleep.\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "使用中文\n",
      "---------- ThoughtEvent (assistant) ----------\n",
      "The user is asking me to use Chinese. They previously asked me to write a 4-line poem about the ocean, and I provided one in English. Now they want me to write a 4-line poem about the ocean in Chinese. I'll create a simple 4-line poem about the ocean in Chinese.\n",
      "---------- TextMessage (assistant) ----------\n",
      "\n",
      "蔚蓝海洋波涛涌，\n",
      "浪花拍岸声声重。\n",
      "深邃海底藏秘密，\n",
      "千古谜题在其中。\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "请继续\n",
      "---------- ThoughtEvent (assistant) ----------\n",
      "The user is asking me to continue in Chinese. They previously asked for a 4-line poem about the ocean, and I provided one in English and then in Chinese. Now they're asking me to continue, which I interpret as wanting another poem about the ocean in Chinese. I'll write a different 4-line poem about the ocean in Chinese.\n",
      "---------- TextMessage (assistant) ----------\n",
      "\n",
      "海天一色无边际，\n",
      "潮起潮落永不息。\n",
      "鱼儿畅游水世界，\n",
      "自然奇景令人迷。\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "ok\n",
      "---------- ThoughtEvent (assistant) ----------\n",
      "The user said \"ok\", which seems to indicate they are satisfied with the additional 4-line poem I provided in Chinese about the ocean. They haven't asked for anything else, so I should acknowledge their response and wait for further instructions or indicate that the task is complete if they seem satisfied.\n",
      "\n",
      "Since they just said \"ok\" and haven't made any new requests, I'll provide a brief acknowledgment.\n",
      "---------- TextMessage (assistant) ----------\n",
      "\n",
      "好的，如果您还有其他需要帮助的地方，请随时告诉我。\n",
      "---------- TextMessage (user_proxy) ----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for assistant_90c845ad-fdd5-4b54-8d3e-de6a9c45c227/90c845ad-fdd5-4b54-8d3e-de6a9c45c227\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 827, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 955, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1791, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1591, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\n",
      "Error processing publish message for user_proxy_90c845ad-fdd5-4b54-8d3e-de6a9c45c227/90c845ad-fdd5-4b54-8d3e-de6a9c45c227\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 604, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 133, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1791, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1591, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m stream = team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mWrite a 4-line poem about the ocean.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Use asyncio.run(...) when running in a script.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(stream)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model_client.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:518\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    515\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    516\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    519\u001b[39m     stop_reason = message.message.content\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\nTraceback:\nTraceback (most recent call last):\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 827, in on_messages_stream\n    async for inference_output in self._call_llm(\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 955, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 624, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2454, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1791, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/userroot/miniconda3/envs/agent/lib/python3.12/site-packages/openai/_base_client.py\", line 1591, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.BadRequestError: Error code: 400 - {'error': {'code': '1213', 'message': '未正常接收到prompt参数。'}}\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "#model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
    "termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# Create the team.\n",
    "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
    "\n",
    "# Run the conversation and stream to the console.\n",
    "stream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n",
    "# Use asyncio.run(...) when running in a script.\n",
    "await Console(stream)\n",
    "await model_client.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
